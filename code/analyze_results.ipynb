{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/bondarenko/laptop/Jena/research/ACQuA/ratio-24/ratio24-new-cam\n",
      "/home/bondarenko\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import classification_report\n",
    "from pathlib import Path \n",
    "import os\n",
    "\n",
    "#set current working directory to the root\n",
    "if not os.getcwd().endswith('ratio24-new-cam'):\n",
    "    os.chdir('../')\n",
    "print(os.getcwd())\n",
    "print(Path.home())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BETTER       0.85      0.87      0.86       273\n",
      "        NONE       0.95      0.94      0.94      1048\n",
      "       WORSE       0.68      0.71      0.70       119\n",
      "\n",
      "    accuracy                           0.91      1440\n",
      "   macro avg       0.83      0.84      0.83      1440\n",
      "weighted avg       0.91      0.91      0.91      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "INPUT_PATH = Path(\"results\")\n",
    "\n",
    "df_eng = pd.read_csv(os.path.join(INPUT_PATH, 'roberta_5epoch_preds.tsv'), sep='\\t')\n",
    "df_eng[\"preds\"].replace({0: \"NONE\", 1: \"BETTER\", 2: \"WORSE\"}, inplace=True)\n",
    "df_eng[\"labels\"].replace({0: \"NONE\", 1: \"BETTER\", 2: \"WORSE\"}, inplace=True)\n",
    "print(classification_report(y_true=df_eng.labels, y_pred=df_eng.preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5759 1440 7199\n",
      "0:  0.73, 2:  0.19, 3:  0.08\n"
     ]
    }
   ],
   "source": [
    "INPUT_PATH = Path(\"data\")\n",
    "\n",
    "df_train = pd.read_csv(os.path.join(INPUT_PATH, \"comparg_train.tsv\"), sep='\\t', encoding='utf-8')  \n",
    "df_test = pd.read_csv(os.path.join(INPUT_PATH, \"comparg_test.tsv\"), sep='\\t', encoding='utf-8')\n",
    "\n",
    "print(len(df_train), len(df_test), len(df_train)+len(df_test))\n",
    "labels = df_test.labels.tolist()\n",
    "print(f\"0: {labels.count(0)/len(labels): .2f}, 2: {labels.count(2)/len(labels): .2f}, 3: {labels.count(3)/len(labels): .2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1208\n",
      "NONE:  0.704, BETTER:  0.213, WORSE:  0.084\n"
     ]
    }
   ],
   "source": [
    "INPUT_PATH = Path(\"data\")\n",
    "\n",
    "df_test = pd.read_csv(os.path.join(INPUT_PATH, \"ru_sentences_with_objs.tsv\"), sep='\\t', encoding='utf-8')\n",
    "print(len(df_test))\n",
    "labels = df_test.tag.tolist()\n",
    "print(f\"NONE: {labels.count('NONE')/len(labels): .3f}, BETTER: {labels.count('BETTER')/len(labels): .3f}, WORSE: {labels.count('WORSE')/len(labels): .3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min: 1, max: 87\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BETTER       0.86      0.89      0.87       273\n",
      "        NONE       0.95      0.95      0.95      1048\n",
      "       WORSE       0.71      0.69      0.70       119\n",
      "\n",
      "    accuracy                           0.91      1440\n",
      "   macro avg       0.84      0.84      0.84      1440\n",
      "weighted avg       0.91      0.91      0.91      1440\n",
      "\n"
     ]
    }
   ],
   "source": [
    "INPUT_PATH = Path(\"data\")\n",
    "df_res = pd.read_csv(os.path.join(INPUT_PATH, '5_epochs_comparg_test.tsv'), sep='\\t')\n",
    "# df_train = pd.read_csv(os.path.join(INPUT_PATH, 'comparg_train.tsv'), sep='\\t')\n",
    "df_res[\"pred_labels\"].replace({0: \"NONE\", 2: \"BETTER\", 3: \"WORSE\"}, inplace=True)\n",
    "df_res[\"labels\"].replace({0: \"NONE\", 2: \"BETTER\", 3: \"WORSE\"}, inplace=True)\n",
    "df_res[\"answer_words\"] = df_res[\"answer\"].apply(lambda x: len(x.split()))\n",
    "print(f\"min: {df_res.answer_words.min()}, max: {df_res.answer_words.max()}\")\n",
    "print(classification_report(y_true=df_res.labels, y_pred=df_res.pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BETTER       0.96      0.90      0.93        50\n",
      "        NONE       0.98      1.00      0.99       257\n",
      "       WORSE       0.67      0.57      0.62         7\n",
      "\n",
      "    accuracy                           0.97       314\n",
      "   macro avg       0.87      0.82      0.84       314\n",
      "weighted avg       0.97      0.97      0.97       314\n",
      "\n",
      "0.22\n"
     ]
    }
   ],
   "source": [
    "y_true = df_res[(df_res.answer_words >=1) & (df_res.answer_words <= 10)].labels\n",
    "y_pred = df_res[(df_res.answer_words >=1) & (df_res.answer_words <= 10)].pred_labels\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(round(len(y_true)/len(df_res), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BETTER       0.86      0.89      0.88       199\n",
      "        NONE       0.95      0.94      0.94       612\n",
      "       WORSE       0.74      0.73      0.73       100\n",
      "\n",
      "    accuracy                           0.91       911\n",
      "   macro avg       0.85      0.85      0.85       911\n",
      "weighted avg       0.91      0.91      0.91       911\n",
      "\n",
      "0.63\n"
     ]
    }
   ],
   "source": [
    "y_true = df_res[(df_res.answer_words > 10) & (df_res.answer_words <= 30)].labels\n",
    "y_pred = df_res[(df_res.answer_words > 10) & (df_res.answer_words <= 30)].pred_labels\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(round(len(y_true)/len(df_res), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      BETTER       0.65      0.83      0.73        24\n",
      "        NONE       0.94      0.91      0.92       179\n",
      "       WORSE       0.45      0.42      0.43        12\n",
      "\n",
      "    accuracy                           0.87       215\n",
      "   macro avg       0.68      0.72      0.69       215\n",
      "weighted avg       0.88      0.87      0.87       215\n",
      "\n",
      "0.15\n"
     ]
    }
   ],
   "source": [
    "y_true = df_res[(df_res.answer_words > 30)].labels\n",
    "y_pred = df_res[(df_res.answer_words > 30)].pred_labels\n",
    "print(classification_report(y_true, y_pred))\n",
    "print(round(len(y_true)/len(df_res), 2))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
